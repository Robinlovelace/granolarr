---
title: "Lecture 502<br/>Univariate analysis"
author: "Dr Stefano De Sabbata<br/>School of Geography, Geology, and the Env.<br/><a href=\"mailto:s.desabbata@le.ac.uk\">s.desabbata&commat;le.ac.uk</a> &vert; <a href=\"https://twitter.com/maps4thought\">&commat;maps4thought</a><br/><a href=\"https://github.com/sdesabbata/GY7702\">github.com/sdesabbata/GY7702</a> licensed under <a href=\"https://www.gnu.org/licenses/gpl-3.0.html\">GNU GPL v3.0</a>"
date: "`r Sys.Date()`"
output:
  ioslides_presentation:
    template: ../Utils/IOSlides/UoL_Template.html
    logo: ../Utils/IOSlides/uol_logo.png
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
knitr::opts_knit$set(root.dir = rprojroot::find_rstudio_root_file())
rm(list = ls())
```

<style type="text/css">
.small_r_all pre{
  font-size: 16px;
  line-height: 18px;
}
.small_r_output pre:not(.prettyprint){
  font-size: 16px;
  line-height: 18px;
}
.verysmall_r_output pre:not(.prettyprint){
  font-size: 12px;
  line-height: 14px;
}
</style>


# Recap @ 502



## Previous lectures

- Introduction to R
- Data types
- Data wrangling
- Reproducibility
- Exploratory analysis
    - Data visualisation
    - Descriptive statistics
    - Exploring assumptions



## This lecture

- Comparing means
    - t-test
    - ANOVA
- Correlation
    - Pearson's r
    - Spearman's rho
    - Kendall's tau
- Regression
    - univariate



# Lecture 502<br/>Comparing means



## Libraries

<div class="columns-2">

Today's libraries

- mostly working with the usual `nycflights13`
- exposition pipe `%$%` from the library `magrittr`

```{r, echo=TRUE, message=FALSE, warning=FALSE,}
library(tidyverse)
library(magrittr)  
library(nycflights13)
```

But let's start from a simple example from `datasets`

- 50 flowers from each of 3 species of iris

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 3.5, fig.height = 5}
iris %>%
  ggplot(
    aes(
      x = Species, 
      y = Petal.Length
    )
  ) +
  geom_boxplot()
```

</div>



## Example

<div class="small_r_all">

```{r, echo=TRUE, message=FALSE, warning=FALSE}
iris %>% filter(Species == "setosa") %>% pull(Petal.Length) %>% shapiro.test()

iris %>% filter(Species == "versicolor") %>% pull(Petal.Length) %>% shapiro.test()

iris %>% filter(Species == "virginica") %>% pull(Petal.Length) %>% shapiro.test()
```

</div>



## T-test

Independent T-test tests whether two group means are different

$$outcome_i = (group\ mean) + error_i $$

- groups defined by a predictor, categorical variable
- outcome is a continuous variable
- assuming
    - normally distributed values in groups
    - homogeneity of variance of values in groups
        - if groups have different sizes
    - independence of groups



## Example

Values are normally distributed, groups have same size, and they are independent (different flowers, check using `leveneTest`)

<div class="small_r_output">

```{r, echo=TRUE, message=FALSE, warning=FALSE}
iris %>%
  filter(Species %in% c("versicolor", "virginica")) %$% # Note %$%
  t.test(Petal.Length ~ Species)
```

</div>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
iris_t_test <- iris %>%
  filter(Species %in% c("versicolor", "virginica")) %$%
  t.test(Petal.Length ~ Species)
```

The difference is significant t(`r iris_t_test %$% parameter[["df"]] %>% round(digits = 2)`) = `r iris_t_test %$% statistic[["t"]] %>% round(digits = 2)`, *p* < .01



## ANOVA

ANOVA (analysis of variance) tests whether more than two group means are different

$$outcome_i = (group\ mean) + error_i $$

- groups defined by a predictor, categorical variable
- outcome is a continuous variable
- assuming
    - normally distributed values in groups
        - especially if groups have different sizes
    - homogeneity of variance of values in groups
        - if groups have different sizes
    - independence of groups



## Example

Values are normally distributed, groups have same size, they are independent (different flowers, check using `leveneTest`)

<div class="small_r_output">

<!--
```{r, echo=TRUE, message=FALSE, warning=FALSE}
library(car)

iris %$%
leveneTest(Petal.Length, Species)
```
-->

```{r, echo=TRUE, message=FALSE, warning=FALSE}
iris %$%
  aov(Petal.Length ~ Species) %>%
  summary()
```

</div>

```{r, echo=FALSE, message=FALSE, warning=FALSE}
iris_anova_summary <- iris %$%
  aov(Petal.Length ~ Species) %>%
  summary()
```

The difference is significant t(`r iris_anova_summary[[1]] %$% Df[1] %>% round(digits = 2)`, `r iris_anova_summary[[1]] %$% Df[2] %>% round(digits = 2)`) = `r iris_anova_summary[[1]][1, 4] %>% round(digits = 2)`, *p* < .01




# Lecture 502<br/>Correlation



## Correlation

Two variables can be related in three different ways

- related
    - positively: entities with high values in one tend to have high values in the other
    - negatively: entities with high values in one tend to have low values in the other
- not related at all

**Correlation** is a standardised measure of covariance



## Example

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
flights_nov_20 <- nycflights13::flights %>%
  filter(!is.na(dep_delay), !is.na(arr_delay), month == 11, day ==20) 
```

<center>
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay, y = arr_delay)) +
  geom_point() + coord_fixed(ratio = 1)
```
</center>


## Example

<div class="small_r_output">

```{r, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  pull(dep_delay) %>% shapiro.test()

flights_nov_20 %>%
  pull(arr_delay) %>% shapiro.test()
```

</div>



## Pearson’s r

<div class="columns-2 small_r_output">

If  two variables are **normally distributed**, use **Pearson's r**

```{r, echo=FALSE, message=FALSE, warning=FALSE}
flights_nov_20_cor <- flights_nov_20 %$%
  cor.test(dep_delay, arr_delay)
```

The square of the correlation value indicates the percentage of shared variance

*If they were normally distributed, but they are not* 

- `r flights_nov_20_cor$estimate %>% round(digits = 3)` ^ 2 = `r flights_nov_20_cor$estimate ^ 2 %>% round(digits = 3)`
- departure and arrival delay *would* share `r ((flights_nov_20_cor$estimate ^ 2) * 100) %>% round(digits = 1)`% of variance

<br/><br/>

```{r, echo=TRUE, message=FALSE, warning=FALSE}
# note the use of %$%
#instead of %>%
flights_nov_20 %$%
  cor.test(dep_delay, arr_delay)
```

</div>



## Spearman’s rho

<div class="columns-2 small_r_output">

```{r, echo=FALSE, message=FALSE, warning=FALSE}
flights_nov_20_cor_spearman <- flights_nov_20 %$%
  cor.test(dep_delay, arr_delay, method = "spearman")
```

If  two variables are **not normally distributed**, use **Spearman’s rho**

- non-parametric
- based on rank difference

The square of the correlation value indicates the percentage of shared variance

*If few ties, but there are*

- `r flights_nov_20_cor_spearman$estimate %>% round(digits = 3)` ^ 2 = `r flights_nov_20_cor_spearman$estimate ^ 2 %>% round(digits = 3)`
- departure and arrival delay *would* share `r ((flights_nov_20_cor_spearman$estimate ^ 2) * 100) %>% round(digits = 1)`% of variance


```{r, echo=TRUE, message=FALSE}
flights_nov_20 %$%
  cor.test(
    dep_delay, arr_delay, 
    method = "spearman")
```

</div>



## Kendall’s tau

<div class="columns-2 small_r_output">

```{r, echo=FALSE, message=FALSE, warning=FALSE}
flights_nov_20_cor_kendall <- flights_nov_20 %$%
  cor.test(dep_delay, arr_delay, method = "kendall")
```

If **not normally distributed** and there is a **large number of ties**, use **Kendall’s tau**

- non-parametric
- based on rank difference

The square of the correlation value indicates the percentage of shared variance

**Departure and arrival delay seem actually to share**

- `r flights_nov_20_cor_kendall$estimate %>% round(digits = 3)` ^ 2 = `r flights_nov_20_cor_kendall$estimate ^ 2 %>% round(digits = 3)`
- **`r ((flights_nov_20_cor_kendall$estimate ^ 2) * 100) %>% round(digits = 1)`% of variance**


```{r, echo=TRUE, message=FALSE}
flights_nov_20 %$%
  cor.test(
    dep_delay, arr_delay, 
    method = "kendall")
```

</div>



## Pairs plot

Combines in one visualisation: histograms, scatter plots, and correlation values for a set of variables

<div class="columns-2">

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
library(psych)

flights_nov_20 %>%
  select(
    dep_delay, 
    arr_delay, 
    air_time
  ) %>%
  pairs.panels(
    method = "kendall"
  )
```

<center>
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 4}
library(psych)

flights_nov_20 %>%
  select(dep_delay, arr_delay, air_time) %>%
  pairs.panels(method = "kendall")
```
</center>

</div>



<!--
# Lecture 502<br/>Data transformations


## Z-scores

*Z-scores* transform the values as relative to the distribution mean and standard deviation

<div class="columns-2">

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  mutate(
    dep_delay_zscore = 
      scale(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_zscore)
  ) +
  geom_histogram()
```

<br/>

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  mutate(
    dep_delay_zscore = scale(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_zscore)
  ) +
  geom_histogram()
```

</div>



## Log transformation

*Logarithmic* transformations (e.g., `log` and `log10`) are useful to *"un-skew"* variables, but only possible on values `> 0`

<div class="columns-2">

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  filter(dep_delay > 0) %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  filter(dep_delay > 0) %>%
  mutate(
    dep_delay_log = 
      log(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_log)) + 
  geom_histogram()
```

<br/>

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  filter(dep_delay > 0) %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  filter(dep_delay > 0) %>%
  mutate(
    dep_delay_log = 
      log(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_log)) + 
  geom_histogram()
```

</div>



## Inverse hyperbolic sine


*Inverse hyperbolic sine* (`asinh`) transformations are useful to *"un-skew"* variables, similar to logarithmic transformations, work on all values

<div class="columns-2">

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE}
flights_nov_20 %>%
  mutate(
    dep_delay_ihs = 
      asinh(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_ihs)) + 
  geom_histogram()
```

<br/>

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay)) +
  geom_histogram()
```

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 4, fig.height = 2}
flights_nov_20 %>%
  mutate(
    dep_delay_ihs = 
      asinh(dep_delay)
  ) %>%
  ggplot(
    aes(x = dep_delay_ihs)) + 
  geom_histogram()
```

</div>
-->



# Lecture 502<br/> Regression



## Regression analysis

**Regression analysis** is a supervised machine learning approach

Predict the value of one outcome variable as

$$outcome_i = (model) + error_i $$

- one predictor variable (**simple / univariate** regression)

$$Y_i = (b_0 + b_1 * X_{i1}) + \epsilon_i $$
    
- more predictor variables (**multiple / multivariate** regression)

$$Y_i = (b_0 + b_1 * X_{i1} + b_2 * X_{i2} + \dots + b_M * X_{iM}) + \epsilon_i $$



## Least squares

<div class="columns-2">

**Least squares** is the most commonly used approach to generate a regression model

The model fits a line
    
- to minimise the squared values of the **residuals** (errors)
- that is squared difference between
    - **observed values**
    - **model**


<center>
![](Images/489px-Linear_least_squares_example2.svg.png){width=70%}

<br/>
<font size="4">	
by 	Krishnavedala<br/>
via Wikimedia Commons,<br/>CC-BY-SA-3.0
</font>
</center>

</div>

$$deviation = \sum(observed - model)^2$$



## Example

<font size="4">	
$$arr\_delay_i = (b_0 + b_1 * dep\_delay_{i1}) + \epsilon_i $$
</font>

<div class="small_r_output">

```{r, echo=TRUE}
delay_model <- flights_nov_20 %$% # Note %$%
  lm(arr_delay ~ dep_delay)

delay_model %>%  summary()
```

</div>



## Overall fit

```{r, echo=FALSE}
delay_model_summary <- delay_model %>%
  summary()
```

The output indicates

- **p-value: < 2.2e-16**: $p<.001$ the model is significant
    - derived by comparing the calulated **F-statistic** value to F distribution `r delay_model_summary$fstatistic[1] %>% round(digits = 2)` having specified degrees of freedom (`r delay_model_summary$fstatistic[2]`, `r delay_model_summary$fstatistic[3]`)
    - Report as: F(`r delay_model_summary$fstatistic[2]`, `r delay_model_summary$fstatistic[3]`) = `r delay_model_summary$fstatistic[1] %>% round(digits = 2)`
- **Adjusted R-squared: `r delay_model_summary$adj.r.squared %>% round(digits = 4)`**: the departure delay can account for `r (delay_model_summary$adj.r.squared * 100) %>% round(digits = 2)`% of the arrival delay
- **Coefficients**
    - Intercept estimate `r delay_model_summary$coefficients[1,1] %>% round(digits = 4)` is significant
    - `dep_delay` (slope) estimate `r delay_model_summary$coefficients[2,1] %>% round(digits = 4)` is significant



## Parameters

<font size="4">	
$$arr\_delay_i = (Intercept + Coefficient_{dep\_delay} * dep\_delay_{i1}) + \epsilon_i $$
</font>

```{r, eval=FALSE, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay, y = arr_delay)) +
  geom_point() + coord_fixed(ratio = 1) +
  geom_abline(intercept = 4.0943, slope = 1.04229, color="red")
```

<center>
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
flights_nov_20 %>%
  ggplot(aes(x = dep_delay, y = arr_delay)) +
  geom_point() + coord_fixed(ratio = 1) +
  geom_abline(intercept = 4.0943, slope = 1.04229, color="red")
```
</center>


<!--
## Outliers and residuals
## Influential cases
-->



## Checking assumptions

- **Linearity**
    - the relationship is actually linear
- **Normality** of residuals
    - standard residuals are normally distributed with mean `0`
- **Homoscedasticity** of residuals
    - at each level of the predictor variable(s) the variance of the standard residuals should be the same (*homo-scedasticity*) rather than different (*hetero-scedasticity*) 
- **Independence** of residuals
    - adjacent standard residuals are not correlated
- When more than one predictor: **no multicollinearity**
    - if two or more predictor variables are used in the model, each pair of variables not correlated



## Normality

Shapiro-Wilk test for normality of standard residuals, 

- robust models: should be not significant 

<div class="columns-2">

```{r, echo=TRUE, message=FALSE, warning=FALSE}
delay_model %>% 
  rstandard() %>% 
  shapiro.test()
```

<font size="4">	
**Standard residuals are NOT normally distributed**
</font>

<center>
```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 4}
delay_model %>% 
  rstandard() %>%
  data.frame(std_res = .) %>%
  ggplot(aes(x = std_res)) +
  geom_histogram(
    aes(
      y =..density..
    ),
    bins = 100
  ) + 
  stat_function(
    fun = dnorm, 
    args = list(
      mean = delay_model %>% rstandard() %>% mean(),
      sd = delay_model %>% rstandard() %>% sd()),
    colour = "red", size = 1)
```
</center>

</div>



## Homoscedasticity

Breusch-Pagan test for homoscedasticity of standard residuals

- robust models: should be not significant

<div class="columns-2 small_r_output">

```{r, echo=TRUE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
library(lmtest)

delay_model %>% 
  bptest()
```

<font size="4">	
**Standard residuals are homoscedastic**
</font>

```{r, echo=FALSE, message=FALSE, warning=FALSE, fig.width = 3, fig.height = 3}
delay_model %>% 
  plot(which = c(1))
```

</div>



## Independence

Durbin-Watson test for the independence of residuals

- robust models: statistic should be close to 2 (between 1 and 3) and not significant

<div class="small_r_output">

```{r, echo=TRUE}
# Also part of the library lmtest
delay_model %>%
  dwtest()
```

</div>

<font size="4">	
**Standard residuals might not be completely indipendent**

Note: the result depends on the order of the data.
</font>



# Lecture 502<br/>Summary



## Summary

- Comparing means
    - t-test
    - ANOVA
- Correlation
    - Pearson's r
    - Spearman's rho
    - Kendall's tau
- Regression
    - univariate



## Practical session

In the practical session, we will see:

- Comparing means
    - ANOVA
- Regression
    - univariate
    - multivariate



## Next lecture

- Machine Learning
    - Definition
    - Types
- Unsupervised
    - Clustering
- In GIScience
    - Geodemographic classification